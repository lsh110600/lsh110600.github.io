---
layout: post
title: 'Multomodal deep learning - CMU ê°•ì˜ 2.1 Basic Concepts'
subtitle: '2.1 Basic Concepts'
categories: deeplearning
tags: multimodal
comments: true
---


ê°•ì˜  2.1 Basic Conceptsì— ëŒ€í•œ ë‚´ìš© ì •ë¦¬. 


Unimodal basic representations & core concept of nueral network

## Unimodal basic representation
1. Unimodal Classification â€“ Visual Modality
![Capture](/assets/img/post/multimodal/2021-1-9-multimodal-2.JPG)

- ì´ë¯¸ì§€ê°€ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ëª¨ë©´, ê°ê°ì˜ í”½ì…€ë“¤ì€ RGB 3ì°¨ì› ê³µê°„ì— í‘œí˜„ë¨.(Gray scale ì´ë¯¸ì§€ëŠ” 2ì°¨ì›.) 
- ì…ë ¥ ì´ë¯¸ì§€ê°€ Dogì´ëƒ? ì•„ë‹ˆëƒ? íŒë‹¨í•˜ëŠ” ë¬¸ì œëŠ” `Binary Classification problem`
- ì…ë ¥ ì´ë¯¸ì§€ê°€ Dogì´ëƒ? Duckì´ëƒ? Catì´ëƒ? ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œëŠ”, `Multi-class classification problem`
- ì…ë ¥ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ê°œì˜ Objectê°€ ìˆë‹¤ë©´, {Dog, Bird}ì´ëƒ? {Dog, Duck}ì´ëƒ? {Cat, Pig}ì´ëƒ? ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œëŠ” `Multi-label (or multi-task) classification problem`
- ì…ë ¥ ì´ë¯¸ì§€ì˜ Objectì˜ AgeëŠ”? WeightëŠ”? í–‰ë³µí•´ë³´ì´ëŠ”ì§€? ê°’ì„ ë‚´ëŠ” ë¬¸ì œëŠ” `Multi-label (or multi-task) regression problem`
- ëª¨ë¸ì˜ ì…ë ¥ ê°’ xëŠ” vectorì„ì„ ì•Œê³ ìˆì. 

2. Unimodal Classification â€“ Language Modality
![Capture](/assets/img/post/multimodal/2021-1-9-multimodal-3.JPG)
- LanguageëŠ” Written language(í‚¤ë³´ë“œë¡œ íƒ€ì…í•‘ëœ í˜•íƒœ)ì™€ Spoken language(ëŒ€í™”ë¥¼ Textë¡œ ì˜®ê¸´ê²ƒ)ë¡œ êµ¬ë¶„ë¨. 
- ì‰½ê²Œ ë§í•˜ë©´, paragraph ì•ˆì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ Encodeí•˜ëŠ” ê³¼ì •. 

3. Unimodal Classification â€“ Acoustic Modality
![Capture](/assets/img/post/multimodal/2021-1-9-multimodal-4.JPG)
- ìŒì„± ì‹ í˜¸ì²˜ë¦¬ ê³¼ì •ì„ ì„¤ëª…. Bit depthê°€ í´ ìˆ˜ë¡ more fine-grainedí•œ dataë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ.
- time windowsë¡œ ì–´ë–»ê²Œ sliceë¥¼ í•˜ëŠëƒê°€ ì¤‘ìš”. 
- ì–¼ë§ˆë‚˜ ë†’ì€(í˜¹ì€ ë‚®ì€) ì£¼íŒŒìˆ˜ë¥¼ ê°€ì¡ŒëŠ”ì§€ëŠ” spectogramì„ í†µí•´ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆìŒ.(yì¶•ì´ frequency)
- spectogramì˜ ì „ì²´ë¥¼ ë‹¤ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë³´ë‹¤, window ë³„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•¨. 

ê¸°íƒ€ ë‹¤ë¥¸ Unimodal Classificationìœ¼ë¡œëŠ” Graph, set, tableì„ ì‚¬ìš©í•œ ê²ƒì´ ìˆë‹¤. 

## Data-Driven Machine Learning 
- Dataset: Collection of labeled samples D:{ğ‘¥_ğ‘–, ğ‘¦_ğ‘–}
- Training: Learn classifier on training set
- Testing: Evaluate classifier on hold-out test set
- Validation: Select optimal hyper-parameters (same size with test set)

### Nearest Neighbor Classifier
- Non-parametric approachesâ€”key ideas
  - Predict new cases based on similar cases
  - Use multiple local models instead of a single global model
- "Distance metrics"ë¥¼ ì‚¬ìš©. (L1 or L2 distance)
- ì–´ë–¤ metricì„ ì“°ì§€?, Kë¥¼ ì–´ë–»ê²Œ ì„¤ì •í•˜ì§€? => hyper-parameter ì„ íƒ
  
### Evaluation methods (for validation and testing)
- Holdout set: The available data set D is divided into two disjoint subsets(training set, test set)
- n-fold cross-validation: The available data is partitioned into n equal-size disjoint subsets. 

## Linear Classification: Scores and Loss
- ë”¥ëŸ¬ë‹ ê¸°ì´ˆëŠ” ìì„¸íˆ ì„¤ëª…í•˜ì§„ ì•Šê² ìŠµë‹ˆë‹¤.(loss function, activation function ë“±)
1. Define a (linear) score function
2. Define the loss function (possibly nonlinear)
3. Optimization

- ì…ë ¥ xì™€ ê°€ì¤‘ì¹˜ Wë¥¼ ê³±í•˜ê³  biasí•­ì„ ë”í•œ í•¨ìˆ˜ fë¥¼ ì‚¬ìš©í•˜ì—¬ scoreë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. 
- Normalizeí•œ í›„ ê°€ì¥ ë†’ì€ scoreë¥¼ ì •ë‹µìœ¼ë¡œ ì •í•œ í›„ labelê³¼ ë¹„êµí•©ë‹ˆë‹¤. 
 -The loss function quantifies the amount by which the prediction scores deviate from the actual values



ê¸°ê³„í•™ìŠµ, ë‰´ëŸ´ë„·ì— ëŒ€í•œ basic ë‚´ìš©. 
